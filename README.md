# Retail Sales Optimization - Capstone Project

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![MongoDB](https://img.shields.io/badge/MongoDB-Compatible-green.svg)
![AWS](https://img.shields.io/badge/AWS-S3%20%7C%20DocumentDB-orange.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ“‹ Project Overview

This capstone project focuses on **Retail Sales Optimization** for ABC Retail Corp, implementing a comprehensive data engineering pipeline to ingest, clean, transform, and analyze large-scale retail datasets. The project generates actionable business insights through advanced analytics and machine learning models.

**Dataset Source:** [Customer Shopping Dataset - Kaggle](https://www.kaggle.com/datasets/mehmettahiraslan/customer-shopping-dataset)

**Students** 
Nitin Bhoyate
Jai Kishore Rana
Bhaskar Adhikary
Vivek Kumar Srivastava
Vaibhav Darekar

## ğŸ¯ Objectives

1. **Data Ingestion & Storage**
   - Load data from both AWS S3 and local filesystem
   - Store raw and processed data in MongoDB-compatible database
   - Support for multiple data formats (CSV, Excel, Parquet)

2. **Data Cleaning & Quality**
   - Handle missing values, outliers, and inconsistent formats
   - Maintain comprehensive data quality logs
   - Standardize dates, text, and categorical data

3. **Exploratory Data Analysis (EDA)**
   - Analyze sales patterns and seasonal trends
   - Evaluate product performance across categories
   - Understand customer behavior and demographics

4. **Feature Engineering & Modeling**
   - Create time-series features (lags, rolling windows)
   - Engineer customer-level features (RFM analysis)
   - Build predictive models for sales forecasting

5. **Data Warehousing & Optimization**
   - Design optimized MongoDB schema with indexing
   - Implement aggregation pipelines for fast queries
   - Export curated datasets in Parquet format

6. **Visualization & Reporting**
   - Interactive Streamlit dashboard
   - Comprehensive reports and presentations
   - Real-time business insights

## ğŸ—ï¸ Project Structure

```
Retail-Sales-Optimization-Group-9/
â”‚
â”œâ”€â”€ config/                          # Configuration files
â”‚   â””â”€â”€ config.yaml                  # Main configuration
â”‚
â”œâ”€â”€ scripts/                         # Core Python scripts
â”‚   â”œâ”€â”€ utils/                       # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config_loader.py        # Config & client initialization
â”‚   â”‚   â”œâ”€â”€ logger.py               # Logging utilities
â”‚   â”‚   â””â”€â”€ db_operations.py        # MongoDB operations
â”‚   â”‚
â”‚   â”œâ”€â”€ data_ingestion/             # Data loading scripts
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ load_from_s3.py         # S3 data loader
â”‚   â”‚   â”œâ”€â”€ load_from_local.py      # Local data loader
â”‚   â”‚   â””â”€â”€ data_loader.py          # Unified data loader
â”‚   â”‚
â”‚   â”œâ”€â”€ data_cleaning/              # Data cleaning pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ cleaning_pipeline.py    # Cleaning & preprocessing
â”‚   â”‚
â”‚   â””â”€â”€ etl/                        # ETL & transformation
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ feature_engineering.py  # Feature creation
â”‚       â”œâ”€â”€ transformation_pipeline.py
â”‚       â””â”€â”€ aggregation.py          # MongoDB aggregations
â”‚
â”œâ”€â”€ notebooks/                       # Jupyter notebooks
â”‚   â”œâ”€â”€ eda/
â”‚   â”‚   â””â”€â”€ EDA_sales.ipynb         # Exploratory analysis
â”‚   â””â”€â”€ modeling/
â”‚       â””â”€â”€ modeling_forecasting.ipynb  # ML models
â”‚
â”œâ”€â”€ dashboards/                      # Visualization dashboards
â”‚   â””â”€â”€ app.py                      # Streamlit dashboard
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ SCHEMA_DESIGN.md           # MongoDB schema design
â”‚   â””â”€â”€ USER_GUIDE.md              # User guide
â”‚
â”œâ”€â”€ dataset/                        # Local data storage
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ cleaned/
â”‚   â””â”€â”€ transformed/
â”‚
â”œâ”€â”€ logs/                           # Application logs
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ .env.example                    # Environment variables template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9 or higher
- MongoDB (local or AWS DocumentDB)
- AWS Account (optional, for S3 storage)
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd Retail-Sales-Optimization-Group-9
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

5. **Download the dataset**
   - Download from [Kaggle](https://www.kaggle.com/datasets/mehmettahiraslan/customer-shopping-dataset)
   - Place the `shopping.csv` file in `dataset/raw/`

### Configuration

Edit `config/config.yaml` to customize:
- MongoDB connection settings
- AWS S3 bucket names
- Data cleaning parameters
- Feature engineering settings
- Model configurations

## ğŸ“Š Usage

### 1. Data Ingestion

**Load from local file:**
```bash
cd scripts/data_ingestion
python data_loader.py --source local --path ../../dataset/customer_shopping_data.csv --collection raw_sales --kaggle-dataset
```

**Load from S3:**
```bash
python data_loader.py --source s3 --path raw_data/customer_shopping_data.csv --collection raw_sales
```

### 2. Data Cleaning

```bash
cd scripts/data_cleaning
python cleaning_pipeline.py --input-collection raw_sales --output-collection cleaned_sales --shopping-dataset
```

### 3. Data Transformation

```bash
cd scripts/etl
python transformation_pipeline.py --input-collection cleaned_sales --output-collection transformed_sales --create-aggregations
```

### 4. Run Jupyter Notebooks

```bash
jupyter notebook notebooks/eda/EDA_sales.ipynb
jupyter notebook notebooks/modeling/modeling_forecasting.ipynb
```

### 5. Launch Dashboard

```bash
cd dashboards
streamlit run app.py
```

The dashboard will be available at `http://localhost:8501`

### Run single click Pipeline
```bash
python run_pipeline.py
```

## ğŸ› ï¸ Tech Stack

### Core Technologies
- **Python 3.9+**: Primary programming language
- **MongoDB**: NoSQL database for flexible data storage
- **AWS S3**: Object storage for raw and processed data
- **AWS DocumentDB**: Managed MongoDB-compatible database (optional)

### Data Processing
- **pandas**: Data manipulation and analysis
- **numpy**: Numerical computing
- **scikit-learn**: Machine learning algorithms
- **statsmodels**: Statistical models and time series
- **Prophet**: Time series forecasting

### Visualization
- **matplotlib**: Static plotting
- **seaborn**: Statistical data visualization
- **plotly**: Interactive visualizations
- **Streamlit**: Web-based dashboards

### AWS & Cloud
- **boto3**: AWS SDK for Python
- **pymongo**: MongoDB driver for Python

### Development Tools
- **Jupyter**: Interactive notebooks
- **pytest**: Testing framework
- **python-dotenv**: Environment variable management

## ğŸ“ˆ Key Features

### Data Ingestion
- âœ… Dual-mode loading (S3 + Local)
- âœ… Support for CSV, Excel, Parquet
- âœ… Batch processing capabilities
- âœ… Error handling and logging

### Data Cleaning
- âœ… Automated missing value imputation
- âœ… Outlier detection and handling
- âœ… Date standardization
- âœ… Text normalization
- âœ… Duplicate removal
- âœ… Comprehensive cleaning logs

### Feature Engineering
- âœ… Time-based features (lag, rolling windows)
- âœ… Customer features (RFM analysis)
- âœ… Product/category features
- âœ… Aggregate statistics
- âœ… Categorical encoding

### Machine Learning
- âœ… Linear Regression
- âœ… Random Forest Regressor
- âœ… XGBoost (configurable)
- âœ… ARIMA/Prophet for time series
- âœ… Model evaluation metrics (RMSE, MAE, RÂ²)

### Dashboard Features
- âœ… Real-time KPI monitoring
- âœ… Interactive sales trends
- âœ… Category performance analysis
- âœ… Customer demographics insights
- âœ… Shopping mall comparison
- âœ… Time-based analysis
- âœ… Data export functionality

## ğŸ“Š Dataset Information

**Source:** [Customer Shopping Dataset - Kaggle](https://www.kaggle.com/datasets/mehmettahiraslan/customer-shopping-dataset)

**Description:** The dataset contains shopping information from 10 different shopping malls in Istanbul, Turkey between 2021 and 2023.

**Columns:**
- `invoice_no`: Invoice number
- `customer_id`: Customer ID
- `gender`: Customer gender
- `age`: Customer age
- `category`: Product category
- `quantity`: Number of items purchased
- `price`: Price per item
- `payment_method`: Payment method used
- `invoice_date`: Date of purchase
- `shopping_mall`: Shopping mall location

**Size:** ~100,000+ transactions

## ğŸ” MongoDB Schema Design

See [docs/SCHEMA_DESIGN.md](docs/SCHEMA_DESIGN.md) for detailed schema documentation.

### Collections:
- `raw_sales`: Original raw data
- `cleaned_sales`: Cleaned and standardized data
- `transformed_sales`: Feature-engineered data
- `aggregated_sales`: Pre-aggregated metrics
- `agg_daily_sales`: Daily aggregations
- `agg_category_sales`: Category-level metrics
- `agg_customer_summary`: Customer-level summaries

## ğŸ“ Documentation

- **[Schema Design](docs/SCHEMA_DESIGN.md)**: MongoDB collection schemas
- **[User Guide](docs/USER_GUIDE.md)**: Comprehensive usage guide
- **Configuration**: See `config/config.yaml` for all settings

## ğŸ§ª Testing

Run tests (when implemented):
```bash
pytest tests/
```

## ğŸ“ˆ Performance Optimization

### MongoDB Indexing
```python
# Create indexes for faster queries
db_handler.create_index('cleaned_sales', [('invoice_date', 1)])
db_handler.create_index('cleaned_sales', [('customer_id', 1)])
db_handler.create_index('cleaned_sales', [('category', 1)])
```

### Aggregation Pipeline
Utilize MongoDB's aggregation framework for efficient data processing.

### Parquet Export
Export large datasets to Parquet format for:
- 50-80% compression ratio
- Columnar storage benefits
- Fast analytical queries

## ğŸ¤ Contributing

This is an academic capstone project. For contributions or suggestions:

1. Fork the repository
2. Create a feature branch
3. Commit changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“§ Contact

For questions or support, please contact the project team.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Dataset**: Mehmet Tahir Aslan - Kaggle
- **Institution**: IIT Jodhpur - M.Tech Data Science
- **Course**: Capstone Project
- **Academic Year**: 2025

## ğŸ“š References

1. Customer Shopping Dataset - Kaggle: https://www.kaggle.com/datasets/mehmettahiraslan/customer-shopping-dataset
2. MongoDB Documentation: https://docs.mongodb.com/
3. AWS S3 Documentation: https://docs.aws.amazon.com/s3/
4. Streamlit Documentation: https://docs.streamlit.io/
5. scikit-learn Documentation: https://scikit-learn.org/

---

**Built with â¤ï¸ for Retail Sales Optimization**

